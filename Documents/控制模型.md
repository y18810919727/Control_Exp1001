# 控制模型

## 控制基类



所有控制类的父类，为了满足控制算法模型的通用型，该部分定义的借接口比较少。

1. 计算控制指令

```python
def _act(self, state):
	raise NotImplementedError
```

该方法必须重现，根据当前state计算控制指令u

2. 模型训练

```python
def _train(self, s, u, ns, r, done):
	raise NotImplementedError
```

该方法必须重现，输入一个$(s,u,ns,r,done)$对，模型进行训练

## Actor-critic网络基类

定义了使用梯度策略类网络的通用结构

1. 计算控制指令

```python
def policy_act(self, state):
	raise NotImplementedError
```

该方法必须重现，根据当前state计算控制指令u

> 与_act(self, state)的区别： Actor-critic基类重写了_act方法，根据train_mode是否为true，添加探索噪音，policy_act 方法只需要关注于如何根据state输出一个控制指令即可，如果想在policy网络中添加噪音，需要在定义controller类时使用No_Exploration类型噪音探索器。

## 样例控制模型

此处根据样例仿真模型设计了一个最优控制器，继承了ControlBase类

```python

# -*- coding:utf8 -*-
from Control_Exp1001.control.base_control import ControlBase

import numpy as np

class DemoControl(ControlBase):

    def __init__(self, u_bounds):
        super(DemoControl,self).__init__(u_bounds)

    def _act(self, state):
        y_star = state[0:2]
        y = state[2:4]
        u = np.clip(y_star - y, self.u_bounds[:,0], self.u_bounds[:, 1])
        return u

    def _train(self, s, u, ns, r, done):
        pass

```

## 主流ac网络实现

### td3

- 论文

> Fujimoto, S., van Hoof, H., & Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. https://doi.org/10.1089/lrb.2012.0022

- 参考代码：

> https://github.com/higgsfield/RL-Adventure-2/blob/master/7.soft%20actor-critic.ipynb

- 创建方法

```
    def __init__(self,
                 gpu_id=0,
                 num_inputs=None,
                 num_actions=None,
                 act_hidden_size=None,
                 val_hidden_size=None,
                 replay_buffer=None,
                 u_bounds=None,
                 exploration=None,
                 batch_size=64,
                 policy_lr=1e-3,
                 value_lr=1e-3,
                 noise_std=0.2,
                 noise_clip = 0.5,
                 gamma=0.99,
                 policy_update=2,
                 soft_tau=1e-3
                 ):

```

- 参数详解
  - gpu_id: 使用gpu id
  - num_inputs: 观测变量维度
  - num_actions: 控制维度
  - act_hidden_size: 策略网络隐层节点数
  - val_hidden_size: 评估网络隐层节点数
  - replay_buffer: 经验回放池对象
  - u_bounds: 控制指令上下界
  - exploration: 噪音探索器
  - batch_size: 每次训练从回放池选取的样本数
  - policy_lr: 策略网络学习率
  - value_lr: 评估网络学习率
  - noise_std: 为next_action添加的高斯噪音的方差
  - noise_clip: 噪音上下限
  - gamma: 累积奖赏的折扣因子
  - policy_update: 策略网络更新周期
  - soft_tau: 当前网络向target网络更新的速率