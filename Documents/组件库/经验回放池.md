# 经验回放池

DQN的一大核心革命性的改进在于引入经验回放池技术，随着与环境的交互产生新的经验，旧的经验并不丢掉而是共同作用于模型的更新，此乃"温故而知新"道理。从算法原理上看，可以有效地削弱由于马尔科夫决策模型导致短期内的$state$陷入于某一局部，与整体状态空间的分布相悖，进而导致Q网络陷入局部最优的情况。

## 目前包含的经验回放池种类

- 普通经验回放池

## 普通经验回放池

- 论文

  >  Mnih, V., Silver, D., & Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. *Nips*, 1–9. https://doi.org/10.1038/nature14236

1. 构建

   - 对象初始化源码

   ```python
   def __init__(self, capacity)
   ```

   - 参数详解:
     - capacity : 回放池大小
   - 例子

   ```python
   from Control_Exp1001.common.replay.replay_buffer import ReplayBuffer
   replay_buffer = ReplayBuffer(1000)
   ```

2. 添加$(s,r,a,ns)$

   - 方法

- ```python
  def push(self, state, action, reward, next_state, done):
  ```

  - 参数详解:
    - state: 当前状态
    - action: 执行控制指令
    - reward: 单步奖赏
    - next_state: 下个状态
    - done: 是否结束

3. 采样
   - 源码

- ```python
  def sample(self, batch_size, step=None):
      ...
      return state, action, reward, next_state, done
  ```

  - 参数详解:
    - batch_size: 采样个数
    - step: 当前round下已经运行到多少步
    - 返回值: s,a,r,ns,done的多维nparray，每个行数为batch_size

## 